import gradio as gr
import numpy as np
import soundfile as sf
from datetime import datetime
from time import time as ttime
from my_utils import load_audio
from transformers import pipeline
from text.cleaner import clean_text
from polyglot.detect import  Detector
from feature_extractor import cnhubert
from timeit import default_timer as timer
from text import cleaned_text_to_sequence
from module.models  import  SynthesizerTrn
from module.mel_processing import spectrogram_torch
from transformers.pipelines.audio_utils import ffmpeg_read
import os,re,sys,LangSegment,librosa,pdb,torch,pytz,random
from transformers import AutoModelForMaskedLM, AutoTokenizer
from AR.models.t2s_lightning_module import Text2SemanticLightningModule


import logging
logging.getLogger("markdown_it").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("httpcore").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("asyncio").setLevel(logging.ERROR)
logging.getLogger("charset_normalizer").setLevel(logging.ERROR)
logging.getLogger("torchaudio._extension").setLevel(logging.ERROR)
logging.getLogger("multipart").setLevel(logging.WARNING)
from download import *
download()
from TTS_infer_pack.TTS import TTS, TTS_Config
from TTS_infer_pack.text_segmentation_method import get_method

if "_CUDA_VISIBLE_DEVICES" in os.environ:
    os.environ["CUDA_VISIBLE_DEVICES"] = os.environ["_CUDA_VISIBLE_DEVICES"]
tz = pytz.timezone('Asia/Singapore')
device = "cuda" if torch.cuda.is_available() else "cpu"

def abs_path(dir):
    global_dir = os.path.dirname(os.path.abspath(sys.argv[0]))
    return(os.path.join(global_dir, dir))
gpt_path = abs_path("MODELS/22/22.ckpt")
sovits_path=abs_path("MODELS/22/22.pth")
cnhubert_base_path = os.environ.get("cnhubert_base_path", "pretrained_models/chinese-hubert-base")
bert_path = os.environ.get("bert_path", "pretrained_models/chinese-roberta-wwm-ext-large")

if not os.path.exists(cnhubert_base_path):
    cnhubert_base_path = "TencentGameMate/chinese-hubert-base"
if not os.path.exists(bert_path):
    bert_path = "hfl/chinese-roberta-wwm-ext-large"
cnhubert.cnhubert_base_path = cnhubert_base_path

whisper_path = os.environ.get("whisper_path", "pretrained_models/whisper-tiny")
if not os.path.exists(whisper_path):
    whisper_path = "openai/whisper-tiny"

pipe = pipeline(
    task="automatic-speech-recognition",
    model=whisper_path,
    chunk_length_s=30,
    device=device,)


is_half = eval(
    os.environ.get("is_half", "True" if torch.cuda.is_available() else "False")
)


dict_language = {
    "ä¸­æ–‡1": "all_zh",
    "English": "en",
    "æ—¥æ–‡1": "all_ja",
    "ä¸­æ–‡": "zh",
    "æ—¥æœ¬èª": "ja",
    "æ··åˆ": "auto",
}

cut_method = {
    "Do not split/ä¸åˆ‡":"cut0",
    "Split into groups of 4 sentences/å››å¥ä¸€åˆ‡": "cut1",
    "Split every 50 characters/50å­—ä¸€åˆ‡": "cut2",
    "Split at CN/JP periods (ã€‚)/æŒ‰ä¸­æ—¥æ–‡å¥å·åˆ‡": "cut3",
    "Split at English periods (.)/æŒ‰è‹±æ–‡å¥å·åˆ‡": "cut4",
    "Split at punctuation marks/æŒ‰æ ‡ç‚¹åˆ‡": "cut5",
}


tts_config = TTS_Config("GPT_SoVITS/configs/tts_infer.yaml")
tts_config.device = device
tts_config.is_half = is_half
if gpt_path is not None:
    tts_config.t2s_weights_path = gpt_path
if sovits_path is not None:
    tts_config.vits_weights_path = sovits_path
if cnhubert_base_path is not None:
    tts_config.cnhuhbert_base_path = cnhubert_base_path
if bert_path is not None:
    tts_config.bert_base_path = bert_path

    
tts_pipline = TTS(tts_config)
gpt_path = tts_config.t2s_weights_path
sovits_path = tts_config.vits_weights_path


def inference(text, text_lang, 
              ref_audio_path, prompt_text, 
              prompt_lang, top_k, 
              top_p, temperature, 
              text_split_method, batch_size, 
              speed_factor, ref_text_free,
              split_bucket,
              volume
              ):

    if not duration(ref_audio_path):
        return None
    if  text == '':
        wprint("Please input text to generate/è¯·è¾“å…¥ç”Ÿæˆæ–‡å­—")
        return None
    text=trim_text(text,text_language)             
    try:
        lang=dict_language[text_lang]
        inputs={
        "text": text,
        "text_lang": lang,
        "ref_audio_path": ref_audio_path,
        "prompt_text": prompt_text if not ref_text_free else "",
        "prompt_lang": dict_language[prompt_lang],
        "top_k": top_k,
        "top_p": top_p,
        "temperature": temperature,
        "text_split_method": cut_method[text_split_method],
        "batch_size":int(batch_size),
        "speed_factor":float(speed_factor),
        "split_bucket":split_bucket,
        "volume":volume,
        "return_fragment":False,
        }
    
        yield next(tts_pipline.run(inputs))
    except KeyError as e:
        wprint(f'Unsupported language type:{e}')
        return None

#==========custom functions============

splits = {"ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", ",", ".", "?", "!", "~", ":", "ï¼š", "â€”", "â€¦", }
def tprint(text):
    now=datetime.now(tz).strftime('%H:%M:%S')
    print(f'UTC+8 - {now} - {text}')

def wprint(text):
    tprint(text)
    gr.Warning(text)

def lang_detector(text):
    min_chars = 5
    if len(text) < min_chars:
        return "Input text too short/è¾“å…¥æ–‡æœ¬å¤ªçŸ­"
    try:
        detector = Detector(text).language
        lang_info = str(detector)
        code = re.search(r"name: (\w+)", lang_info).group(1)
        if code == 'Japanese':
            return "æ—¥æœ¬èª"
        elif code == 'Chinese':
            return "ä¸­æ–‡"
        elif code == 'English':
            return 'English'
        else:
            return code
    except Exception as e:
        return f"ERRORï¼š{str(e)}"
        
def trim_text(text,language): 
    limit_cj = 120 #character
    limit_en = 60 #words  
    search_limit_cj = limit_cj+30
    search_limit_en = limit_en +30
    text = text.replace('\n', '').strip()
    
    if language =='English':
        words = text.split()
        if len(words) <= limit_en:
            return text
        # English
        for i in range(limit_en, -1, -1):
            if any(punct in words[i] for punct in splits):
                return ' '.join(words[:i+1])
        for i in range(limit_en, min(len(words), search_limit_en)):
            if any(punct in words[i] for punct in splits):
                return ' '.join(words[:i+1])
        return ' '.join(words[:limit_en])
        
    else:
        if len(text) <= limit_cj:
            return text
        for i in range(limit_cj, -1, -1):  
            if text[i] in splits:
                return text[:i+1]
        for i in range(limit_cj, min(len(text), search_limit_cj)):  
            if text[i] in splits:
                return text[:i+1]
        return text[:limit_cj]   

def duration(audio_file_path):
    if not audio_file_path:
        wprint("Failed to obtain uploaded audio/æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return False
    try:
        audio_duration = librosa.get_duration(filename=audio_file_path)
        if not 3 < audio_duration < 10:
            wprint("The audio length must be between 3~10 seconds/éŸ³é¢‘æ—¶é•¿é¡»åœ¨3~10ç§’ä¹‹é—´")
            return False
        return True
    except FileNotFoundError:
        return False

def update_model(choice):
    #global tts_config.vits_weights_path, tts_config.t2s_weights_path
    model_info = models[choice]
    gpt_path = abs_path(model_info["gpt_weight"])
    sovits_path = abs_path(model_info["sovits_weight"])
    tts_pipline.init_vits_weights(sovits_path)
    tts_pipline.init_t2s_weights(gpt_path)
    model_name = choice
    tone_info = model_info["tones"]["tone1"] 
    tone_sample_path = abs_path(tone_info["sample"])
    tprint(f'âœ…SELECT MODELï¼š{choice}')
    # è¿”å›é»˜è®¤toneâ€œtone1â€
    return (
        tone_info["example_voice_wav"],   
        tone_info["example_voice_wav_words"],   
        model_info["default_language"],   
        model_info["default_language"],
        model_name,
        "tone1"  ,
        tone_sample_path
    )

def update_tone(model_choice, tone_choice):
    model_info = models[model_choice]  
    tone_info = model_info["tones"][tone_choice]  
    example_voice_wav = abs_path(tone_info["example_voice_wav"])  
    example_voice_wav_words = tone_info["example_voice_wav_words"]  
    tone_sample_path = abs_path(tone_info["sample"])
    return example_voice_wav, example_voice_wav_words,tone_sample_path

def transcribe(voice):
    time1=timer()
    tprint('âš¡Start Clone - transcribe')
    task="transcribe"
    if voice is None:
        wprint("No audio file submitted! Please upload or record an audio file before submitting your request.")
    R = pipe(voice, batch_size=8, generate_kwargs={"task": task}, return_timestamps=True,return_language=True)
    text=R['text']
    lang=R['chunks'][0]['language']
    if lang=='english':
      language='English'
    elif lang =='chinese':
      language='ä¸­æ–‡'
    elif lang=='japanese':
      language = 'æ—¥æœ¬èª'

    time2=timer()
    tprint(f'transcribe COMPLETE,{round(time2-time1,4)}s')
    tprint(f'  \nTranscribe resultï¼š\n ğŸ”£Languageï¼š{language} \n ğŸ”£Textï¼š{text}' )
    return  text,language  

def clone_voice(user_voice,user_text,user_lang):
    if not duration(user_voice):
        return None
    if  user_text == '':
        wprint("Please enter text to generate/è¯·è¾“å…¥ç”Ÿæˆæ–‡å­—")
        return None
    user_text=trim_text(user_text,user_lang)
    global gpt_path, sovits_path
    gpt_path = abs_path("pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt")
    #tprint(f'Model loaded:{gpt_path}')
    sovits_path = abs_path("pretrained_models/s2G488k.pth")
    #tprint(f'Model loaded:{sovits_path}')
    try:
        prompt_text, prompt_lang = transcribe(user_voice)
    except UnboundLocalError as e:
        wprint(f"The language in the audio cannot be recognized ï¼š{str(e)}")
        return None
    tts_pipline.init_vits_weights(sovits_path)
    tts_pipline.init_t2s_weights(gpt_path)
    inputs={
        "text": user_text,
        "text_lang": dict_language[user_lang],
        "ref_audio_path": user_voice,
        "prompt_text": prompt_text,
        "prompt_lang": dict_language[prompt_lang],
        "top_k": 5,
        "top_p": 1,
        "temperature": 1,
        "text_split_method": "cut1",
        "batch_size":20,
        "speed_factor":1.0,
        "split_bucket":True,
        "volume":1.0,
        "return_fragment":False,
    }
  
    yield next(tts_pipline.run(inputs))

with open('dummy') as f:
    dummy_txt = f.read().strip().splitlines()

def dice():
    return random.choice(dummy_txt), 'ğŸ²'

from info import models
models_by_language = {
    "English": [],
    "ä¸­æ–‡": [],
    "æ—¥æœ¬èª": []
}
for model_name, model_info in models.items():
    language = model_info["default_language"]
    models_by_language[language].append((model_name, model_info))

##########GRADIO###########

with gr.Blocks(theme='Kasien/ali_theme_custom') as app:
    gr.HTML('''
  <h1 style="font-size: 25px;">TEXT TO SPEECH</h1>
  <h1 style="font-size: 20px;">Support English/Chinese/Japanese</h1>
  <p style="margin-bottom: 10px; font-size: 100%">
   If you like this space, please click the â¤ï¸ at the top of the page..å¦‚å–œæ¬¢ï¼Œè¯·ç‚¹ä¸€ä¸‹é¡µé¢é¡¶éƒ¨çš„â¤ï¸<br>
  </p>''')

    gr.Markdown("""* This space is based on the text-to-speech generation solution [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) . 
    You can visit the repo's github homepage to learn training and inference.<br>
    æœ¬ç©ºé—´åŸºäºæ–‡å­—è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ¡ˆ [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS). ä½ å¯ä»¥å‰å¾€é¡¹ç›®çš„githubä¸»é¡µå­¦ä¹ å¦‚ä½•æ¨ç†å’Œè®­ç»ƒã€‚ 
    * âš ï¸Generating voice is very slow due to using HuggingFace's free CPU in this space. 
    For faster generation, click the Colab icon below to use this space in Colab,
    which will significantly improve the speed.<br>
    ç”±äºæœ¬ç©ºé—´ä½¿ç”¨huggingfaceçš„å…è´¹CPUè¿›è¡Œæ¨ç†ï¼Œå› æ­¤é€Ÿåº¦å¾ˆæ…¢ï¼Œå¦‚æƒ³å¿«é€Ÿç”Ÿæˆï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹çš„Colabå›¾æ ‡ï¼Œ
    å‰å¾€Colabä½¿ç”¨å·²è·å¾—æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ã€‚
    <br>Colabã®ä½¿ç”¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚ã‚ˆã‚Šé€Ÿã„ç”Ÿæˆé€Ÿåº¦ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ 
    *  each model can speak three languages.<br>æ¯ä¸ªæ¨¡å‹éƒ½èƒ½è¯´ä¸‰ç§è¯­è¨€<br>å„ãƒ¢ãƒ‡ãƒ«ã¯3ã¤ã®è¨€èªã‚’è©±ã™ã“ã¨ãŒã§ãã¾ã™ã€‚""")   
    gr.HTML('''<a href="https://colab.research.google.com/drive/1fTuPZ4tZsAjS-TrhQWMCb7KRdnU8aF6j" target="_blank"><img src="https://camo.githubusercontent.com/dd83d4a334eab7ada034c13747d9e2237182826d32e3fda6629740b6e02f18d8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6c61622d4639414230303f7374796c653d666f722d7468652d6261646765266c6f676f3d676f6f676c65636f6c616226636f6c6f723d353235323532" alt="colab"></a>
''')

    default_voice_wav, default_voice_wav_words, default_language, _, default_model_name, _, default_tone_sample_path = update_model("Trump")
    english_models = [name for name, _ in models_by_language["English"]]
    chinese_models = [name for name, _ in models_by_language["ä¸­æ–‡"]]
    japanese_models = [name for name, _ in models_by_language["æ—¥æœ¬èª"]]
    with gr.Row():
        english_choice = gr.Radio(english_models, label="EN",value="Trump",scale=3)
        chinese_choice = gr.Radio(chinese_models, label="ZH",scale=2)
        japanese_choice = gr.Radio(japanese_models, label="JA",scale=4)

    plsh='Supportã€English/ä¸­æ–‡/æ—¥æœ¬èªã€‘ï¼ŒInput text you like / è¼¸å…¥æ–‡å­— /ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹'
    limit='Max 70 words. Excess will be ignored./å•æ¬¡æœ€å¤šå¤„ç†120å­—å·¦å³ï¼Œå¤šä½™çš„ä¼šè¢«å¿½ç•¥'

    gr.HTML('''
    <b>Input Text/è¾“å…¥æ–‡å­—</b>''')
    with gr.Row():
        with gr.Column(scale=2): 
            model_name = gr.Textbox(label="Seleted Model/å·²é€‰æ¨¡å‹", value=default_model_name, scale=1) 
            text_language = gr.Textbox(
            label="Language for input text/ç”Ÿæˆè¯­è¨€",
            info='Automatic detection of input language type.',scale=1,interactive=False
            ) 
        text = gr.Textbox(label="INPUT TEXT", lines=5,placeholder=plsh,info=limit,scale=10,min_width=0)
        ddice= gr.Button('ğŸ²', variant='tool',min_width=0,scale=0)

        ddice.click(dice, outputs=[text, ddice])
        text.change( lang_detector, text, text_language)


    with gr.Row():
        with gr.Column(scale=2):    
            tone_select = gr.Radio(
            label="Select Tone/é€‰æ‹©è¯­æ°”",
            choices=["tone1","tone2","tone3"],
            value="tone1",
            info='Tone influences the emotional expression ',scale=1)
        tone_sample=gr.Audio(label="ğŸ”ŠPreview tone/è¯•å¬è¯­æ°” ", scale=8)


    with gr.Accordion(label="prpt voice", open=False,visible=False):
        with gr.Row(visible=True):
            inp_ref = gr.Audio(label="Reference audio", type="filepath", value=default_voice_wav, scale=3)
            prompt_text = gr.Textbox(label="Reference text", value=default_voice_wav_words, scale=3)
            prompt_language = gr.Dropdown(label="Language of the reference audio", choices=["ä¸­æ–‡", "English", "æ—¥æœ¬èª"], value=default_language, scale=1,interactive=False)
            dummy = gr.Radio(choices=["ä¸­æ–‡","English","æ—¥æœ¬èª"],visible=False)
     
    
    with gr.Accordion(label="Additional generation options/é™„åŠ ç”Ÿæˆé€‰é¡¹", open=False):
        with gr.Row():
            how_to_cut = gr.Dropdown(
                label=("How to split input text?/å¦‚ä½•å¯¹è¾“å…¥æ–‡å­—åˆ‡ç‰‡"),
                choices=[("Do not split/ä¸åˆ‡"), ("Split into groups of 4 sentences/å››å¥ä¸€åˆ‡"), ("Split every 50 characters/50å­—ä¸€åˆ‡"), 
                         ("Split at CN/JP periods (ã€‚)/æŒ‰ä¸­æ—¥æ–‡å¥å·åˆ‡"), ("Split at English periods (.)/æŒ‰è‹±æ–‡å¥å·åˆ‡"), ("Split at punctuation marks/æŒ‰æ ‡ç‚¹åˆ‡"), ],
                value=("Split into groups of 4 sentences/å››å¥ä¸€åˆ‡"),
                interactive=True,
            info='A suitable splitting method can achieve better generation results/é€‚åˆçš„åˆ‡ç‰‡æ–¹æ³•ä¼šå¾—åˆ°æ›´å¥½çš„æ•ˆæœ'
            )
            split_bucket = gr.Checkbox(label="Split bucket/æ•°æ®åˆ†æ¡¶", value=True, info='Speed up the inference process/æå‡æ¨ç†é€Ÿåº¦')
        with gr.Row():
            volume = gr.Slider(minimum=0.5, maximum=5, value=1, step=0.1, label='Volume/éŸ³é‡',info='audio distortion due to excessive volume/å¤§äº†è¦çˆ†éŸ³')
            speed_factor = gr.Slider(minimum=0.25,maximum=4,step=0.05,label="Speed factor",value=1.0,info='Playback speed/æ’­æ”¾é€Ÿåº¦')
            batch_size = gr.Slider(minimum=1,maximum=100,step=1,label="Batch size",value=20,info='The number of sentences for batch inference./å¹¶è¡Œæ¨ç†çš„å¥å­æ•°é‡')
        with gr.Row():
            top_k = gr.Slider(minimum=1,maximum=100,step=1,label="top_k",value=5)
            top_p = gr.Slider(minimum=0,maximum=1,step=0.05,label="top_p",value=1)
            temperature = gr.Slider(minimum=0,maximum=1,step=0.05,label="temperature",value=1)
        ref_text_free = gr.Checkbox(label="REF_TEXT_FREE", value=False, visible=False)
        
        
    
    gr.HTML('''
    <b>Generate Voice/ç”Ÿæˆ</b>''')
    with gr.Row():
        main_button = gr.Button("âœ¨Generate Voice", variant="primary", scale=2)
        output = gr.Audio(label="ğŸ’¾Download it by clicking â¬‡ï¸", scale=6)
        #info = gr.Textbox(label="INFO", visible=True, readonly=True, scale=1)

    gr.HTML('''
    Generation is slower, please be patient and wait/åˆæˆæ¯”è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…<br>
    If it generated silence, please try again./å¦‚æœç”Ÿæˆäº†ç©ºç™½å£°éŸ³ï¼Œè¯·é‡è¯•
    <br><br><br><br>
    <h1 style="font-size: 25px;">Clone custom Voice/å…‹éš†è‡ªå®šä¹‰å£°éŸ³</h1>
    <p style="margin-bottom: 10px; font-size: 100%">Need 3~10s audio.This involves voice-to-text conversion followed by text-to-voice conversion, so it takes longer time<br>
    éœ€è¦3~10ç§’è¯­éŸ³ï¼Œè¿™ä¸ªä¼šæ¶‰åŠè¯­éŸ³è½¬æ–‡å­—ï¼Œä¹‹åå†è½¬è¯­éŸ³ï¼Œæ‰€ä»¥è€—æ—¶æ¯”è¾ƒä¹…
    </p>''')
    
    with gr.Row():
        user_voice = gr.Audio(type="filepath", label="ï¼ˆ3~10sï¼‰Upload or Record audio/ä¸Šä¼ æˆ–å½•åˆ¶å£°éŸ³",scale=3)
        with gr.Column(scale=7): 
            user_lang = gr.Textbox(label="Language/ç”Ÿæˆè¯­è¨€",info='Automatic detection of input language type.',interactive=False)
            with gr.Row():
                user_text= gr.Textbox(label="Text for generation/è¾“å…¥æƒ³è¦ç”Ÿæˆè¯­éŸ³çš„æ–‡å­—", lines=5,placeholder=plsh,info=limit)
                dddice= gr.Button('ğŸ²', variant='tool',min_width=0,scale=0)
       
        dddice.click(dice, outputs=[user_text, dddice])

    user_text.change( lang_detector, user_text, user_lang)

    user_button = gr.Button("âœ¨Clone Voice", variant="primary")
    user_output = gr.Audio(label="ğŸ’¾Download it by clicking â¬‡ï¸")

    gr.HTML('''<div align=center><img id="visitor-badge" alt="visitor badge" src="https://visitor-badge.laobi.icu/badge?page_id=Ailyth/DLMP9" /></div>''')
    
    english_choice.change(update_model, inputs=[english_choice], outputs=[inp_ref, prompt_text, prompt_language,dummy,model_name, tone_select, tone_sample])
    chinese_choice.change(update_model, inputs=[chinese_choice], outputs=[inp_ref, prompt_text, prompt_language, dummy,model_name, tone_select, tone_sample])
    japanese_choice.change(update_model, inputs=[japanese_choice], outputs=[inp_ref, prompt_text, prompt_language,dummy,model_name, tone_select, tone_sample])
    tone_select.change(update_tone, inputs=[model_name, tone_select], outputs=[inp_ref, prompt_text, tone_sample])
    
    main_button.click(
    inference,
    inputs=[text, 
              text_language,
              inp_ref, 
              prompt_text, 
              prompt_language,
              top_k, 
              top_p, 
              temperature, 
              how_to_cut, 
              batch_size, 
              speed_factor, 
              ref_text_free,
              split_bucket,
              volume],
    outputs=[output]
    )

    user_button.click(
    clone_voice,
    inputs=[user_voice,user_text,user_lang],
    outputs=[user_output])

app.launch(share=True, show_api=False).queue(api_open=False)